{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "raw_text = 'He is the king. The king is royal . she is the queen. She is the royal'\n",
    "corpus = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(corpus_raw):\n",
    "    split_sequences = []\n",
    "    sequences = corpus_raw.split('.')\n",
    "    for s in sequences:\n",
    "        split_sequences.append(s.split())\n",
    "    return split_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is', 'the', 'king'],\n",
       " ['the', 'king', 'is', 'royal'],\n",
       " ['she', 'is', 'the', 'queen'],\n",
       " ['she', 'is', 'the', 'royal']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = split_text(corpus)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabs(text):\n",
    "    text = text.replace(\".\", \"\")\n",
    "    words = text.split()\n",
    "    words = set(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictinary(words):\n",
    "    word2int = {}\n",
    "    int2word = {}\n",
    "    \n",
    "    for i, w in enumerate(words):\n",
    "        word2int[w] = i\n",
    "        int2word[i] = w\n",
    "    return word2int, int2word\n",
    "\n",
    "words = get_vocabs(corpus)\n",
    "vocab_size = len(words)\n",
    "\n",
    "word2int, int2word = get_dictinary(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_word_pair(sequence, word_index, window_size):\n",
    "    pairs = []\n",
    "    for nb_word in sequence[max(0, word_index-window_size) : min(word_index + window_size, len(sequence))+1]:\n",
    "        if(nb_word != sequence[word_index]):\n",
    "            pairs.append((sequence[word_index], nb_word))\n",
    "    return pairs\n",
    "    \n",
    "def gen_xy_text(sequences, window_size=2):\n",
    "    word_pairs = []\n",
    "    for sequence in sequences:\n",
    "        for word_index, word in enumerate(sequence):\n",
    "            word_pairs += get_word_pair(sequence, word_index,window_size)\n",
    "    return word_pairs\n",
    "\n",
    "word_pairs = gen_xy_text(sequences, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_word_pairs(word_pairs, word2int= word2int):\n",
    "    digit_pairs = []\n",
    "    for w_pair in word_pairs:\n",
    "        digit_pairs.append((word2int[w_pair[0]], word2int[w_pair[1]] ))\n",
    "    return digit_pairs\n",
    "\n",
    "def convert_digit_pairs(digit_pairs, int2word=int2word):\n",
    "    word_pairs = []\n",
    "    for d_pair in digit_pairs:\n",
    "        word_pairs.append((int2word[d_pair[0]], int2word[d_pair[1]] ))\n",
    "    return word_pairs\n",
    "\n",
    "# converted_word_pairs = convert_digit_pairs(digit_xy)\n",
    "\n",
    "def to_onehot_vector(value, vocab_size):\n",
    "    onehot_vector = np.zeros(vocab_size)\n",
    "    onehot_vector[value] = 1\n",
    "    return onehot_vector\n",
    "\n",
    "\n",
    "digit_xy = convert_word_pairs(word_pairs)\n",
    "x_data = []\n",
    "y_data = []\n",
    "for xy in digit_xy:\n",
    "    x_data.append(to_onehot_vector(xy[0], vocab_size))\n",
    "    y_data.append(to_onehot_vector(xy[1], vocab_size))\n",
    "x_data = np.array(x_data)    \n",
    "y_data = np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pre, y_true):\n",
    "    return tf.reduce_mean( -tf.reduce_sum( y_true * tf.log(y_pre), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 4\n",
    "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "w_1 = tf.Variable(tf.random_normal([vocab_size, embed_dim]))\n",
    "b_1 = tf.Variable(tf.random_normal([embed_dim]))\n",
    "\n",
    "hidden = tf.matmul(x, w_1) + b_1\n",
    "w_2 = tf.Variable(tf.random_normal([embed_dim, vocab_size]))\n",
    "b = tf.Variable(tf.random_normal([vocab_size]))\n",
    "temp_y = tf.matmul(hidden, w_2) + b\n",
    "\n",
    "y_pre = tf.nn.softmax(temp_y)\n",
    "y_true = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "loss = cross_entropy_loss(y_pre, y_true)\n",
    "\n",
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "iters = 1000\n",
    "\n",
    "for i in range(iters):\n",
    "    feed_dict = {x: x_data, y_true: y_data}\n",
    "    session.run(train, feed_dict=feed_dict)\n",
    "    loss_val = session.run(loss, feed_dict=feed_dict)\n",
    "    w1_val = session.run(w_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = session.run(w_1 + b_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eclidean_dist(vec1 , vec2):\n",
    "    return np.sqrt(np.sum(np.square(vec1 - vec2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word, vectors):\n",
    "    i_word = word2int[word]\n",
    "    word_vector = vectors[i_word]\n",
    "    min_index = 0\n",
    "    min_dist = 0\n",
    "    while(True):\n",
    "        min_index = randint(0, len(vectors)-1)\n",
    "        if(min_index != i_word):\n",
    "            min_dist = eclidean_dist(word_vector, vectors[min_index])\n",
    "            break\n",
    "    for i, v in enumerate(vectors):\n",
    "        dist = eclidean_dist(word_vector, v)\n",
    "        if( i != i_word and dist < min_dist):\n",
    "            min_index = i\n",
    "            min_dist = dist\n",
    "    return min_index, min_dist\n",
    "word = 'royal'\n",
    "min_index, min_dist = find_closest(word, vectors)\n",
    "print(int2word[min_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "\n",
    "model = TSNE(n_components=2)\n",
    "normalizer = preprocessing.Normalizer()\n",
    "\n",
    "x_reduce = model.fit_transform(vectors)\n",
    "x_reduce = normalizer.fit_transform(x_reduce, 'l2')\n",
    "print(x_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_space(embed_vector):\n",
    "    plt.figure()\n",
    "    plt.ylim(ymax=1,ymin=-1 )\n",
    "    plt.xlim(xmax=1, xmin=-1)\n",
    "    for i,v in enumerate(embed_vector):\n",
    "        plt.annotate(int2word[i], xy=(v[0], v[1]))\n",
    "    plt.show()\n",
    "\n",
    "plot_space(x_reduce)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
